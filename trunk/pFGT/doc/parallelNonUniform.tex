
\subsection{Parallel Implementation}
\label{sc:parallelnufgt}

We create a regular grid of FGT boxes partitioned across processors such that 
 {\tt{(a)}} the size of each box is $h = \sqrt{\delta}$, {\tt{(b)}} each processor owns a sub-grid of boxes and
 {\tt{(c)}} each box is owned by an unique processor.  We use PETSc's \cite{petsc-user-ref, petsc-home-page} DA
 module to manage this distributed regular grid. We construct a parallel linear octree such that each leaf
 contains fewer than $m$ points; we use DENDRO \cite{dendro} to do this. We then mark the leaves as either
 ``Expand'' or ``Direct''; this step is embarrassingly parallel. The direct leaves are then partitioned across 
 the processors such that {\tt{(a)}} they are globally sorted in the Morton ordering and {\tt{(b)}} each leaf 
 is owned by an unique processor. Similarly, we partition the expand leaves across the processors. The expand
 and direct leaves are used at different steps in the algorithm and there are collective communications between
 these steps. So, it is better to have good load balance for the expand and direct leaves independently and hence
 we partition the two sets, $T_d$ and $T_e$, independently.

The plane-wave expansions in the S2W step can be computed independently by each processor, without any communication.
However, the expand leaves and their corresponding FGT boxes may belong to different processors. So, the plane-wave 
expansions computed in the S2W step must be sent to the processors that own the corresponding FGT boxes. The owner of
an FGT box will then add the values it receives from other processors to its existing plane-wave expansions, $w_k$. We 
refer to this step as {\textbf{S2W-Comm}}.

In the W2D step, each processor first forms a list of all FGT boxes whose interaction list contains at least one 
target, $y \in T_d$, owned by this processor. These boxes are sent to the respective processors that own them. The
owner of an FGT box returns the plane-wave coefficients for that box to the processors that requested them. After this
communication, each processor independently computes (\ref{eqn:w2d}).

[Rahul - TODO: D2D]

%{\em (iv) D2D.}
%1. For each point (p) within each octant marked as Direct, compute the
%point (A) in the -ve corner and the point (B) in the +ve corner of p's
%interaction list. Note, the Morton ids of all points in the interaction
%list of point p will be >= that of point A and <= that of point B.
%2. Gather the Morton id of the first (in the Morton sorted list) direct
%octant on each processor. This will give the smallest Morton id of
%direct octants on each processor. Handle the case where some processors
%do not contain any direct octants.
%3. Figure out the processor with the greatest value <= A in the list
%computed in step 2.
%4. Figure out the processor with the greatest value <= B in the list
%computed in step 2.
%5. Send point p and the corresponding source f to all processors that
%lie between the processors computed in step 3 and step 4. Note, we are 
%using the property that if the Morton id of Octant C < Morton id of 
%Octant D then the Morton id of any point within Octant C is >  Morton
%id of Octant C and is < Morton id of Octant D.
%6. For each point p recieved in step 5, compute the
%point (A) in the -ve corner and the point (B) in the +ve corner of p's
%interaction list. Find the direct octant O1 with the maximum Morton id that
%is <= A and the direct octant O2 with the maximum Morton id that is <=
%B. Loop over the set of Direct octants O with Morton ids >= that of O1
%and <= that of O2. Loop over the set of points within O that lie in the
%interaction list of p. For each of these points, add the contribution to
%the Gauss transform from p.

 Next, we communicate the plane-wave expansions of the ``{\em ghost}'' boxes\footnote{We refer to any box that
 is owned by a different CPU but lies in the interaction list of some box owned by this CPU as a ghost box.} just
 like in the W2L step of the parallel expansion algorithm (Section \ref{sc:parallelExpansion}). Subsequently, each processor 
 independently executes the rest of the W2L step as described in Section \ref{sc:octreefgt}.

In the D2L step, each processor first computes its contributions to the local expansions of those FGT boxes that intersect
 the interaction list of at least one source contained within some direct leaf owned by this processor; this does not 
 require any communication. Next, we send these contributions to the processors that own the respective FGT boxes. The owner of
an FGT box will then add the values it receives from other processors to its existing local expansions, $v_k$.

The local expansions for each FGT box that overlaps at least one expand leaf are then sent to the processors that own
 the corresponding expand leaf. We refer to this step as {\textbf{L2T-Comm}} and it is like the dual of the S2W-Comm step. 
 
Finally, each processor independently executes the L2T step and computes the transform at all targets within its portion of $T_e$.

We summarize the overall algorithm and give the complexity estimates for the main steps in Algorithm \ref{a:ofgt}.  

\begin{algorithm}[!h]
\caption{ \label{a:ofgt}
\em Parallel FGT for non-uniform distributions}
{\tt
\begin{algorithmic}
\STATE Input: $N$ Points, $\delta$, $\epsilon$, $m$ and $c$
\STATE 1. Create a regular grid of FGT boxes partitioned across processors such that \\
 (A) the size of each box is $h = \sqrt{\delta}$, \\
 (B) each processor owns a sub-grid of boxes and \\
 (C) each box is owned by an unique processor. \\

\STATE 2. Construct a linear octree such that each leaf contains fewer than $m$ points. \\
\hfill $\bigO(\frac{N}{n_p} \log{\frac{N}{n_p}} + n_p \log{n_p})$

\STATE 3. Mark each leaf as either ``expand'' or ``direct'' based on $c$ and $\delta$.

\STATE 4. Partition the direct leaves across processors such that \\
  (A) they are globally sorted in the Morton ordering and \\
  (B) each leaf is owned by an unique processor.

\STATE 5. Partition the expand leaves across processors. 

\STATE 6. Execute S2W. \hfill $\bigO(p^3 \frac{N}{n_p})$

\STATE 7. Execute S2W-Comm. 

\STATE 8. Execute W2D. 

\STATE 9. Execute D2D. 

\STATE 10. Execute W2L. \\
 \hfill $\bigO(p^3 \frac{|B|}{n_p} + K (\frac{|B|}{n_p})^{\frac{2}{3}} + K^2(\frac{|B|}{n_p})^{\frac{1}{3}} + K^3 )$ 

\STATE 11. Execute D2L. 

\STATE 12. Execute L2T-Comm. 

\STATE 13. Execute L2T. \hfill $\bigO( p^3\frac{N}{n_p})$
\end{algorithmic}
}
\end{algorithm}


